name: Predict, Deploy & Auto-Heal Pipeline

on:
  workflow_dispatch:
  push:
    branches: ['main']

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  id-token: write
  contents: read

env:
  AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
  AWS_ACCOUNT_ID:     ${{ secrets.AWS_ACCOUNT_ID }}
  EKS_CLUSTER_NAME:   ${{ secrets.EKS_CLUSTER_NAME }}
  EKS_NAMESPACE:      prod

  BACKEND_REPO:        myapp
  FRONTEND_REPO:       ai-devops-frontend

  PROM_NAMESPACE:      monitoring
  PROM_SERVICE_NAME:   prometheus-operated

  BACKEND_DEPLOY:      backend
  BACKEND_CONTAINER:   backend
  BACKEND_SERVICE:     backend
  FRONTEND_DEPLOY:     frontend
  FRONTEND_CONTAINER:  frontend
  FRONTEND_SERVICE:    frontend

  RISK_THRESHOLD:      "0.70"

  SELECTOR_LABEL:      app
  SELECTOR_VALUE:      backend
  DISABLE_RATE_LIMIT:  true
  METRICS_WAIT_TIME:   120

jobs:
  build-deploy-heal:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Verify model artifact
        run: |
          python3 - <<'PY'
          import os, sys
          p='ml_model/models/model.pkl'
          if not os.path.exists(p):
              print(f"Missing model file: {p}"); sys.exit(1)
          size = os.path.getsize(p)
          with open(p,'rb') as f:
              head=f.read(32)
          if head.startswith(b'version https://'):
              print("LFS POINTER DETECTED: commit real model.pkl (no LFS)"); sys.exit(1)
          if size < 1024:
              print(f"Model file too small/suspicious: {size} bytes"); sys.exit(1)
          print(f"MODEL_FILE_OK size={size} bytes")
          PY

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region:     ${{ env.AWS_DEFAULT_REGION }}

      - name: Who am I?
        run: aws sts get-caller-identity

      - name: Ensure ECR repos
        run: |
          for repo in "${BACKEND_REPO}" "${FRONTEND_REPO}"; do
            aws ecr describe-repositories --repository-names "$repo" >/dev/null 2>&1 \
              || aws ecr create-repository --repository-name "$repo"
          done

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build & push backend
        env:
          IMAGE_URI: ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_DEFAULT_REGION }}.amazonaws.com/${{ env.BACKEND_REPO }}:${{ github.sha }}
        run: |
          docker build --platform linux/amd64 -t backend:prod -f app/Dockerfile .
          docker tag backend:prod "$IMAGE_URI"
          docker push "$IMAGE_URI"
          echo "BACKEND_IMAGE=$IMAGE_URI" >> $GITHUB_ENV

      - name: Build & push frontend
        env:
          IMAGE_URI: ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_DEFAULT_REGION }}.amazonaws.com/${{ env.FRONTEND_REPO }}:${{ github.sha }}
        run: |
          docker build --platform linux/amd64 -t frontend:prod dashboard
          docker tag frontend:prod "$IMAGE_URI"
          docker push "$IMAGE_URI"
          echo "FRONTEND_IMAGE=$IMAGE_URI" >> $GITHUB_ENV

      - name: Upload model artifact (optional)
        run: |
          if [ -f ml_model/models/model.pkl ] && [ -n "${{ secrets.MODEL_S3_PATH }}" ]; then
            aws s3 cp ml_model/models/model.pkl "${{ secrets.MODEL_S3_PATH }}"
          fi

      - name: Install kubectl (pinned)
        run: |
          set -e
          KUBECTL_VERSION=v1.29.6
          curl -fsSL -o kubectl "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl && sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client=true

      - name: Configure kubeconfig
        run: |
          aws eks update-kubeconfig --region "${AWS_DEFAULT_REGION}" --name "${EKS_CLUSTER_NAME}"

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Apply Kubernetes manifests
        run: |
          set -euo pipefail
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/backend-svc.yaml
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/frontend-svc.yaml
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/backend-servicemonitor.yaml || true
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/hpa-backend.yaml || true
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/hpa-frontend.yaml || true
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/ingress.yaml || true

          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/backend-deploy.yaml
          kubectl -n "${EKS_NAMESPACE}" set image deployment/${BACKEND_DEPLOY} ${BACKEND_CONTAINER}="${BACKEND_IMAGE}"

          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/frontend-deploy.yaml
          kubectl -n "${EKS_NAMESPACE}" set image deployment/${FRONTEND_DEPLOY} ${FRONTEND_CONTAINER}="${FRONTEND_IMAGE}"

      - name: Wait for rollouts
        run: |
          set -e
          kubectl -n "${EKS_NAMESPACE}" rollout status deployment/${BACKEND_DEPLOY}  --timeout=5m
          kubectl -n "${EKS_NAMESPACE}" rollout status deployment/${FRONTEND_DEPLOY} --timeout=5m

      - name: Verify ServiceMonitor
        run: |
          echo "ServiceMonitor configuration:"
          kubectl -n "${EKS_NAMESPACE}" get servicemonitor backend -o yaml | grep -A 2 endpoints || true

      - name: Generate sustained traffic
        run: |
          set -euo pipefail
          kubectl apply -n "${EKS_NAMESPACE}" -f - <<'EOF'
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            name: loadgen
          spec:
            replicas: 1
            selector:
              matchLabels: { app: loadgen }
            template:
              metadata: { labels: { app: loadgen } }
              spec:
                containers:
                - name: main
                  image: curlimages/curl:8.11.1
                  command: ["/bin/sh","-c"]
                  args:
                    - >
                      while true; do
                        curl -s "http://backend:5000/healthz" >/dev/null || true;
                        curl -s "http://backend:5000/predict" -H 'Content-Type: application/json'
                        -d '{"features":[0,10,52428800,1,0,0,0]}' >/dev/null || true;
                        sleep 0.2;
                      done
                  resources:
                    requests: { cpu: 10m, memory: 32Mi }
                    limits:   { cpu: 100m, memory: 64Mi }
          EOF
          kubectl -n "${EKS_NAMESPACE}" wait --for=condition=available deploy/loadgen --timeout=60s

      - name: Verify metrics scraping (Prometheus via port-forward)
        run: |
          set -euo pipefail
          kubectl -n "${PROM_NAMESPACE}" get svc "${PROM_SERVICE_NAME}" -o name

          kubectl -n "${PROM_NAMESPACE}" port-forward svc/"${PROM_SERVICE_NAME}" 9090:9090 >/tmp/pf-prom.log 2>&1 &
          PF_PID=$!
          cleanup() { kill $PF_PID 2>/dev/null || true; }
          trap cleanup EXIT

          for i in {1..60}; do curl -sf http://127.0.0.1:9090/-/ready >/dev/null && break || sleep 1; done

          Q='up{namespace="'${EKS_NAMESPACE}'"}'
          RES="$(curl -sG 'http://127.0.0.1:9090/api/v1/query' --data-urlencode "query=$Q")"
          echo "$RES" | jq -e '.status=="success" and (.data.result|length>0)' >/dev/null \
            && echo "✅ Prometheus is scraping targets in ${EKS_NAMESPACE}" \
            || (echo "❌ Prometheus scrape not confirmed"; tail -n 100 /tmp/pf-prom.log; exit 1)

      - name: Predict from runtime metrics
        id: predict
        env:
          NS:         ${{ env.EKS_NAMESPACE }}
          PROM_NS:    ${{ env.PROM_NAMESPACE }}
          PROM_SVC:   ${{ env.PROM_SERVICE_NAME }}
          THRESHOLD:  ${{ env.RISK_THRESHOLD }}
          SEL_VALUE:  ${{ env.SELECTOR_VALUE }}
          RETRY_COUNT: 5
        shell: bash
        run: |
          set -euo pipefail

          start_pf() {
            kubectl -n "$PROM_NS" port-forward svc/"$PROM_SVC" 9090:9090 >/tmp/pf-prom.log 2>&1 &
            echo $! > /tmp/pf_prom.pid
            for i in {1..60}; do
              curl -sf http://127.0.0.1:9090/-/ready >/dev/null && break || sleep 1
            done
          }
          stop_pf() { kill "$(cat /tmp/pf_prom.pid)" 2>/dev/null || true; }

          query_avg() {
            local q="$1"
            local r
            r="$(curl -sG 'http://127.0.0.1:9090/api/v1/query' --data-urlencode "query=$q")"
            echo "$r" | jq '[.data.result[].value[1] | tonumber] | if length>0 then add/length else 0 end'
          }

          attempt=1; max_attempts=${RETRY_COUNT:-3}
          while [ $attempt -le $max_attempts ]; do
            echo "Attempt $attempt/$max_attempts to collect metrics"
            start_pf

            cpu_query="rate(container_cpu_usage_seconds_total{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\",container!=\"POD\"}[2m]) * 100"
            mem_query="avg(container_memory_working_set_bytes{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\",container!=\"POD\"})"
            rst_query="increase(kube_pod_container_status_restarts_total{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\"}[5m])"
            rdy_query="kube_deployment_status_replicas_ready{namespace=\"${NS}\",deployment=\"${BACKEND_DEPLOY}\"}/clamp_min(kube_deployment_spec_replicas{namespace=\"${NS}\",deployment=\"${BACKEND_DEPLOY}\"},1)"
            una_query="kube_deployment_status_replicas_unavailable{namespace=\"${NS}\",deployment=\"${BACKEND_DEPLOY}\"}"
            net_query="rate(container_network_receive_bytes_total{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\"}[2m])"
            err_query="sum(rate(flask_http_request_total{status=~\"5..\",namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\"}[2m]))"

            cpu=$(query_avg "$cpu_query")
            mem=$(query_avg "$mem_query")
            rst=$(query_avg "$rst_query")
            rdy=$(query_avg "$rdy_query")
            una=$(query_avg "$una_query")
            net=$(query_avg "$net_query")
            err=$(query_avg "$err_query")

            features=$(jq -n \
              --argjson cpu "$cpu" \
              --argjson mem "$mem" \
              --argjson rst "$rst" \
              --argjson rdy "$rdy" \
              --argjson una "$una" \
              --argjson net "$net" \
              --argjson err "$err" \
              '{
                restart_count_last_5m: $rst,
                cpu_usage_pct: $cpu,
                memory_usage_bytes: $mem,
                ready_replica_ratio: $rdy,
                unavailable_replicas: $una,
                network_receive_bytes_per_s: $net,
                http_5xx_error_rate: $err
              }')
            echo "Features: $features"

            if jq -e '.[] | select(. > 0)' <<< "$features" >/dev/null; then
              echo "✅ Valid metrics obtained"
              stop_pf
              break
            else
              echo "⚠️ All metrics zero - retrying in 20s"
              stop_pf
              sleep 20
              ((attempt++))
            fi
          done

          if [ $attempt -gt $max_attempts ]; then
            echo "❌ All metric queries returned zero after $max_attempts attempts"
            exit 1
          fi

          # Call backend /predict via local PF
          kubectl -n "${NS}" port-forward svc/${BACKEND_SERVICE} 5000:5000 >/tmp/pf-backend.log 2>&1 &
          B_PID=$!
          for i in {1..30}; do curl -sf http://127.0.0.1:5000/healthz >/dev/null && break || sleep 2; done

          max_retries=3; retry_count=0
          until RESP=$(curl -s -X POST http://127.0.0.1:5000/predict -H "Content-Type: application/json" -d "${features}"); do
            (( retry_count++ ))
            [ $retry_count -ge $max_retries ] && break
            sleep 5
          done

          kill $B_PID 2>/dev/null || true

          echo "Predict: $RESP"
          PROB=$(echo "$RESP" | jq -r '.probability // 0')
          LOADED=$(echo "$RESP" | jq -r '.model_loaded // false')

          echo "fail_prob=$PROB" >> $GITHUB_OUTPUT
          echo "model_loaded=${LOADED}" >> $GITHUB_OUTPUT
          awk -v p="$PROB" -v thr="${THRESHOLD}" 'BEGIN{exit !(p>=thr)}' \
            && echo "highrisk=true"  >> $GITHUB_OUTPUT \
            || echo "highrisk=false" >> $GITHUB_OUTPUT

      - name: Auto-heal (rollout restart)
        if: ${{ steps.predict.outputs.highrisk == 'true' && steps.predict.outputs.model_loaded == 'true' }}
        run: |
          kubectl -n "${EKS_NAMESPACE}" rollout restart deployment/${BACKEND_DEPLOY}
          kubectl -n "${EKS_NAMESPACE}" rollout status  deployment/${BACKEND_DEPLOY} --timeout=5m

      - name: Re-check risk after heal
        id: recheck
        if: ${{ steps.predict.outputs.highrisk == 'true' }}
        env:
          NS:         ${{ env.EKS_NAMESPACE }}
          PROM_NS:    ${{ env.PROM_NAMESPACE }}
          PROM_SVC:   ${{ env.PROM_SERVICE_NAME }}
          THRESHOLD:  ${{ env.RISK_THRESHOLD }}
          SEL_VALUE:  ${{ env.SELECTOR_VALUE }}
        run: |
          set -euo pipefail
          sleep 120

          kubectl -n "$PROM_NS" port-forward svc/"$PROM_SVC" 9090:9090 >/tmp/pf-prom2.log 2>&1 &
          PF2=$!
          for i in {1..60}; do curl -sf http://127.0.0.1:9090/-/ready >/dev/null && break || sleep 1; done

          qavg() { curl -sG 'http://127.0.0.1:9090/api/v1/query' --data-urlencode "query=$1" | jq '[.data.result[].value[1]|tonumber] | if length>0 then add/length else 0 end'; }

          cpu=$(qavg "rate(container_cpu_usage_seconds_total{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\",container!=\"POD\"}[2m]) * 100")
          mem=$(qavg "avg(container_memory_working_set_bytes{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\",container!=\"POD\"})")
          rst=$(qavg "increase(kube_pod_container_status_restarts_total{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\"}[5m])")
          rdy=$(qavg "kube_deployment_status_replicas_ready{namespace=\"${NS}\",deployment=\"${BACKEND_DEPLOY}\"}/clamp_min(kube_deployment_spec_replicas{namespace=\"${NS}\",deployment=\"${BACKEND_DEPLOY}\"},1)")
          una=$(qavg "kube_deployment_status_replicas_unavailable{namespace=\"${NS}\",deployment=\"${BACKEND_DEPLOY}\"}")
          net=$(qavg "rate(container_network_receive_bytes_total{namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\"}[2m])")
          err=$(qavg "sum(rate(flask_http_request_total{status=~\"5..\",namespace=\"${NS}\",pod=~\"${SEL_VALUE}-.*\"}[2m]))")

          FEATURES=$(jq -n --argjson cpu "$cpu" --argjson mem "$mem" --argjson rst "$rst" \
            --argjson rdy "$rdy" --argjson una "$una" --argjson net "$net" --argjson err "$err" \
            '{restart_count_last_5m:$rst,cpu_usage_pct:$cpu,memory_usage_bytes:$mem,ready_replica_ratio:$rdy,unavailable_replicas:$una,network_receive_bytes_per_s:$net,http_5xx_error_rate:$err}')

          kill $PF2 2>/dev/null || true

          kubectl -n "${NS}" port-forward svc/${BACKEND_SERVICE} 5000:5000 >/tmp/pf-backend3.log 2>&1 &
          B3=$!
          for i in {1..30}; do curl -sf http://127.0.0.1:5000/healthz >/dev/null && break || sleep 2; done

          RESP=$(curl -s -X POST http://127.0.0.1:5000/predict -H "Content-Type: application/json" -d "${FEATURES}" || echo '{}')
          kill $B3 2>/dev/null || true

          PROB=$(echo "$RESP" | jq -r '.probability // 0')
          echo "fail_prob2=$PROB" >> $GITHUB_OUTPUT
          awk -v p="$PROB" -v thr="${THRESHOLD}" 'BEGIN{exit !(p>=thr)}' \
            && echo "still_high=true"  >> $GITHUB_OUTPUT \
            || echo "still_high=false" >> $GITHUB_OUTPUT

      - name: Rollback if still high
        if: ${{ steps.recheck.outputs.still_high == 'true' }}
        run: |
          kubectl -n "${EKS_NAMESPACE}" rollout undo deployment/${BACKEND_DEPLOY}

      - name: Slack alert (high → healed)
        if: ${{ steps.predict.outputs.highrisk == 'true' && steps.recheck.outputs.still_high != 'true' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -n "${SLACK_WEBHOOK_URL:-}" ]; then
            msg="⚠️ High risk (p=${{ steps.predict.outputs.fail_prob }}) on ${BACKEND_DEPLOY}. Auto-heal applied."
            printf '%s\n' "$msg" | jq -Rs '{text: .}' | curl -sS -X POST -H 'Content-Type: application/json' --data @- "$SLACK_WEBHOOK_URL"
          fi

      - name: Slack alert (rolled back)
        if: ${{ steps.recheck.outputs.still_high == 'true' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -n "${SLACK_WEBHOOK_URL:-}" ]; then
            msg="⚠️ Risk persisted (p=${{ steps.recheck.outputs.fail_prob2 }}) → rolled back ${BACKEND_DEPLOY}."
            printf '%s\n' "$msg" | jq -Rs '{text: .}' | curl -sS -X POST -H 'Content-Type: application/json' --data @- "$SLACK_WEBHOOK_URL"
          fi

      - name: Slack success (kept)
        if: ${{ steps.predict.outputs.highrisk != 'true' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -n "${SLACK_WEBHOOK_URL:-}" ]; then
            msg="✅ Rollout kept. Risk p=${{ steps.predict.outputs.fail_prob }}."
            printf '%s\n' "$msg" | jq -Rs '{text: .}' | curl -sS -X POST -H 'Content-Type: application/json' --data @- "$SLACK_WEBHOOK_URL"
          fi

      - name: Cleanup resources
        if: always()
        run: |
          kubectl -n "${EKS_NAMESPACE}" delete deployment/loadgen --ignore-not-found
          pkill -f "kubectl.*port-forward" || true
