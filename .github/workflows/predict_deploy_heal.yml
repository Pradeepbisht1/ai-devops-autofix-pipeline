name: Predict, Deploy & Auto-Heal Pipeline

on:
  workflow_dispatch:
  push:
    branches: ['main']

permissions:
  id-token: write
  contents: read

env:
  AWS_DEFAULT_REGION:  ${{ secrets.AWS_DEFAULT_REGION }}
  AWS_ACCOUNT_ID:      ${{ secrets.AWS_ACCOUNT_ID }}
  EKS_CLUSTER_NAME:    ${{ secrets.EKS_CLUSTER_NAME }}
  EKS_NAMESPACE:       prod

  # ECR repos (two separate)
  BACKEND_REPO:        myapp
  FRONTEND_REPO:       ai-devops-frontend

  # Prometheus service (verify with: kubectl -n monitoring get svc)
  PROM_NAMESPACE:      monitoring
  PROM_SERVICE_NAME:   prometheus-operated

  # K8s naming (must match your manifests)
  BACKEND_DEPLOY:      backend
  BACKEND_CONTAINER:   backend
  BACKEND_SERVICE:     backend          # ClusterIP svc on port 5000
  FRONTEND_DEPLOY:     frontend
  FRONTEND_CONTAINER:  frontend
  FRONTEND_SERVICE:    frontend         # LB/Ingress svc on port 80

  RISK_THRESHOLD:      "0.70"

  # === Labels used to select your workload for PromQL ===
  # If your pods have: labels: { app: backend } keep these defaults.
  # If you use app.kubernetes.io/name, change SELECTOR_LABEL accordingly.
  SELECTOR_LABEL:      app
  SELECTOR_VALUE:      backend

jobs:
  build-deploy-heal:
    runs-on: ubuntu-latest

    steps:
      # Checkout (no LFS needed since you committed real model.pkl)
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # ---------- Quick model preflight (no external deps) ----------
      - name: Verify model artifact (no external deps)
        run: |
          python3 - <<'PY'
          import os, sys
          p='ml_model/models/model.pkl'
          if not os.path.exists(p):
              print(f"Missing model file: {p}"); sys.exit(1)
          size = os.path.getsize(p)
          with open(p,'rb') as f:
              head=f.read(32)
          if head.startswith(b'version https://'):
              print("LFS POINTER DETECTED: commit real model.pkl (no LFS)"); sys.exit(1)
          if size < 1024:
              print(f"Model file too small/suspicious: {size} bytes"); sys.exit(1)
          print(f"MODEL_FILE_OK size={size} bytes")
          PY

      # ---------- AWS auth (OIDC role preferred) ----------
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region:     ${{ env.AWS_DEFAULT_REGION }}

      - name: Who am I?
        run: aws sts get-caller-identity

      # ---------- Ensure ECR repos exist ----------
      - name: Ensure ECR repos
        run: |
          for repo in "${BACKEND_REPO}" "${FRONTEND_REPO}"; do
            aws ecr describe-repositories --repository-names "$repo" >/dev/null 2>&1 \
              || aws ecr create-repository --repository-name "$repo"
          done

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      # ---------- Build & push BACKEND image ----------
      - name: Build & push backend
        env:
          IMAGE_URI: ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_DEFAULT_REGION }}.amazonaws.com/${{ env.BACKEND_REPO }}:${{ github.sha }}
        run: |
          docker build --platform linux/amd64 -t backend:prod -f app/Dockerfile .
          docker tag backend:prod "$IMAGE_URI"
          docker push "$IMAGE_URI"
          echo "BACKEND_IMAGE=$IMAGE_URI" >> $GITHUB_ENV

      # ---------- Build & push FRONTEND image ----------
      - name: Build & push frontend
        env:
          IMAGE_URI: ${{ env.AWS_ACCOUNT_ID }}.dkr.ecr.${{ env.AWS_DEFAULT_REGION }}.amazonaws.com/${{ env.FRONTEND_REPO }}:${{ github.sha }}
        run: |
          docker build --platform linux/amd64 -t frontend:prod dashboard
          docker tag frontend:prod "$IMAGE_URI"
          docker push "$IMAGE_URI"
          echo "FRONTEND_IMAGE=$IMAGE_URI" >> $GITHUB_ENV

      # (Optional) model artifact to S3
      - name: Upload model artefact (optional)
        run: |
          if [ -f ml_model/models/model.pkl ] && [ -n "${{ secrets.MODEL_S3_PATH }}" ]; then
            aws s3 cp ml_model/models/model.pkl "${{ secrets.MODEL_S3_PATH }}"
          fi

      # ---------- kubectl ----------
      - name: Install kubectl
        run: |
          curl -sSL https://dl.k8s.io/release/$(curl -sSL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl -o kubectl
          chmod +x kubectl && sudo mv kubectl /usr/local/bin

      - name: Configure kubeconfig
        run: |
          aws eks update-kubeconfig --region "${AWS_DEFAULT_REGION}" --name "${EKS_CLUSTER_NAME}"

      # ---------- Apply manifests (backend, frontend, svc, ingress, hpa, servicemonitor) ----------
      - name: Apply Kubernetes manifests
        run: |
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/backend-svc.yaml
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/frontend-svc.yaml
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/backend-servicemonitor.yaml || true
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/hpa-backend.yaml || true
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/hpa-frontend.yaml || true
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/ingress.yaml || true

          # Deployments (apply first, then set image to our freshly built SHAs)
          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/backend-deploy.yaml
          kubectl -n "${EKS_NAMESPACE}" set image deployment/${BACKEND_DEPLOY} ${BACKEND_CONTAINER}="${BACKEND_IMAGE}"

          kubectl -n "${EKS_NAMESPACE}" apply -f kubernetes/frontend-deploy.yaml
          kubectl -n "${EKS_NAMESPACE}" set image deployment/${FRONTEND_DEPLOY} ${FRONTEND_CONTAINER}="${FRONTEND_IMAGE}"

      # ---------- Wait for rollout ----------
      - name: Wait for backend rollout
        run: kubectl -n "${EKS_NAMESPACE}" rollout status deployment/${BACKEND_DEPLOY} --timeout=5m

      - name: Wait for frontend rollout
        run: kubectl -n "${EKS_NAMESPACE}" rollout status deployment/${FRONTEND_DEPLOY} --timeout=5m

      # ---------- Tooling needed for later steps ----------
      - name: Install jq and Python deps
        run: |
          sudo apt-get update && sudo apt-get install -y jq
          python3 -m pip install --upgrade pip
          python3 -m pip install requests

      # ---------- Warm-up backend (optional) ----------
      - name: Warm-up backend for real metrics
        run: |
          # Port-forward backend service (5000)
          kubectl -n "${EKS_NAMESPACE}" port-forward svc/${BACKEND_SERVICE} 5000:5000 >/tmp/pf-backend.log 2>&1 &
          echo $! > /tmp/pf_backend.pid
          for i in {1..60}; do curl -sf http://127.0.0.1:5000/healthz && break || sleep 1; done
          # 300 quick hits to generate CPU/Net
          for i in {1..300}; do
            curl -s -m 2 http://127.0.0.1:5000/predict -H 'Content-Type: application/json' \
              --data '{"restart_count_last_5m":0,"cpu_usage_pct":10,"memory_usage_bytes":5e7,"ready_replica_ratio":1,"unavailable_replicas":0,"network_receive_bytes_per_s":0,"http_5xx_error_rate":0}' >/dev/null || true
          done
          kill $(cat /tmp/pf_backend.pid) 2>/dev/null || true

      # ---------- Query Prometheus (7 features) & call /predict ----------
      - name: Predict from runtime metrics
        id: predict
        env:
          NS:  ${{ env.EKS_NAMESPACE }}
          PROM_NS:  ${{ env.PROM_NAMESPACE }}
          PROM_SVC: ${{ env.PROM_SERVICE_NAME }}
          THRESHOLD: ${{ env.RISK_THRESHOLD }}
          SEL_LABEL: ${{ env.SELECTOR_LABEL }}
          SEL_VALUE: ${{ env.SELECTOR_VALUE }}
        shell: bash
        run: |
          set -euo pipefail

          # PF Prometheus
          kubectl -n "$PROM_NS" get svc "$PROM_SVC" -o name
          kubectl -n "$PROM_NS" port-forward svc/"$PROM_SVC" 9090:9090 >/tmp/pf-prom.log 2>&1 &
          echo $! > /tmp/pf_prom.pid
          trap 'kill $(cat /tmp/pf_prom.pid) 2>/dev/null || true' EXIT
          for i in {1..60}; do curl -sf http://127.0.0.1:9090/-/ready >/dev/null && break || sleep 1; done

          py() { python3 - "$@"; }

          FEATURES=$(py <<'PY'
          import json,requests,os
          ns=os.getenv("NS")
          label=os.getenv("SEL_LABEL","app")
          val=os.getenv("SEL_VALUE","backend")
          url="http://127.0.0.1:9090/api/v1/query"

          def q(e):
              r=requests.get(url,params={'query':e},timeout=15)
              r.raise_for_status()
              return r.json()

          def val_of(js, default=0.0):
              try:
                  return float(js['data']['result'][0]['value'][1])
              except Exception:
                  return default

          # Normalize label to PromQL form: app.kubernetes.io/name -> label_app_kubernetes_io_name
          lbl = "label_" + label.replace('.', '_').replace('/', '_')
          L = f'kube_pod_labels{{namespace="{ns}", {lbl}="{val}"}}'

          def sum_on_pod(expr):
              return f'sum(({expr}) * on (namespace,pod) group_left({lbl}) ({L}))'

          # Container features
          cpu = val_of(q(sum_on_pod(f'rate(container_cpu_usage_seconds_total{{namespace="{ns}",container!="POD",image!=""}}[2m])'))) * 100
          mem = val_of(q(sum_on_pod(f'container_memory_working_set_bytes{{namespace="{ns}",container!="POD",image!=""}}')))
          net = val_of(q(sum_on_pod(f'rate(container_network_receive_bytes_total{{namespace="{ns}",pod!=""}}[2m])')))
          restarts = val_of(q(sum_on_pod(f'increase(kube_pod_container_status_restarts_total{{namespace="{ns}"}}[5m])')))

          # Deployment-level via deployment labels
          dep_lbl_sum = f'sum by (namespace, deployment, {lbl}) (kube_deployment_labels{{namespace="{ns}", {lbl}="{val}"}})'
          desired = val_of(q(f'sum(kube_deployment_spec_replicas{{namespace="{ns}"}} * on (namespace, deployment) group_left({lbl}) ({dep_lbl_sum}))'))
          ready   = val_of(q(f'sum(kube_deployment_status_ready_replicas{{namespace="{ns}"}} * on (namespace, deployment) group_left({lbl}) ({dep_lbl_sum}))'))
          unavail = val_of(q(f'sum(kube_deployment_status_replicas_unavailable{{namespace="{ns}"}} * on (namespace, deployment) group_left({lbl}) ({dep_lbl_sum}))'))
          ratio = (ready/desired) if desired>0 else 1.0

          # HTTP 5xx fraction (generic -> flask fallback)
          http5xx = 0.0
          for num,den in [
            ('http_requests_total{code=~"5.."}','http_requests_total'),
            ('flask_http_request_total{status=~"5.."}','flask_http_request_total'),
          ]:
              try:
                  n = val_of(q(sum_on_pod(f'rate({num}[2m])')))
                  d = val_of(q('clamp_min('+sum_on_pod(f'rate({den}[2m])')+', 1)'))
                  if d>0 and n/d>0:
                      http5xx = n/d
                      break
              except Exception:
                  pass

          feats={
            "restart_count_last_5m": restarts,
            "cpu_usage_pct": cpu,
            "memory_usage_bytes": mem,
            "ready_replica_ratio": round(ratio,4),
            "unavailable_replicas": unavail,
            "network_receive_bytes_per_s": net,
            "http_5xx_error_rate": http5xx
          }
          print(json.dumps(feats))
          PY
          )

          echo "Features: $FEATURES"

          # Call backend /predict via PF (service, not deployment)
          kubectl -n "${NS}" port-forward svc/${BACKEND_SERVICE} 5000:5000 >/tmp/pf-backend2.log 2>&1 &
          echo $! > /tmp/pf_backend2.pid
          for i in {1..60}; do curl -sf http://127.0.0.1:5000/healthz >/dev/null && break || sleep 1; done

          RESP=$(curl -s -X POST http://127.0.0.1:5000/predict \
                -H "Content-Type: application/json" -d "${FEATURES}")
          echo "Predict: $RESP"

          PROB=$(echo "$RESP" | sed -n 's/.*"probability":\s*\([0-9.]*\).*/\1/p'); : ${PROB:=0}
          LOADED=$(echo "$RESP" | grep -o '"model_loaded":[^,}]*' | cut -d: -f2 | tr -d ' ')

          echo "fail_prob=$PROB" >> $GITHUB_OUTPUT
          echo "model_loaded=${LOADED:-false}" >> $GITHUB_OUTPUT

          # High risk only if model_loaded == true AND prob >= threshold
          if [ "${LOADED:-false}" = "true" ] && awk -v p="$PROB" -v thr="${THRESHOLD}" 'BEGIN{exit !(p>=thr)}'; then
            echo "highrisk=true"  >> $GITHUB_OUTPUT
          else
            echo "highrisk=false" >> $GITHUB_OUTPUT
          fi

          kill $(cat /tmp/pf_backend2.pid) 2>/dev/null || true

      # ---------- Auto-heal (restart) if high risk ----------
      - name: Auto-heal (rollout restart)
        if: ${{ steps.predict.outputs.highrisk == 'true' && steps.predict.outputs.model_loaded == 'true' }}
        run: |
          kubectl -n "${EKS_NAMESPACE}" rollout restart deployment/${BACKEND_DEPLOY}
          kubectl -n "${EKS_NAMESPACE}" rollout status  deployment/${BACKEND_DEPLOY} --timeout=5m

      # ---------- Re-check after heal; rollback if still high ----------
      - name: Re-check risk after heal
        id: recheck
        if: ${{ steps.predict.outputs.highrisk == 'true' }}
        env:
          NS:  ${{ env.EKS_NAMESPACE }}
          PROM_NS:  ${{ env.PROM_NAMESPACE }}
          PROM_SVC: ${{ env.PROM_SERVICE_NAME }}
          THRESHOLD: ${{ env.RISK_THRESHOLD }}
          SEL_LABEL: ${{ env.SELECTOR_LABEL }}
          SEL_VALUE: ${{ env.SELECTOR_VALUE }}
        run: |
          set -euo pipefail
          sleep 60

          # PF prom
          kubectl -n "$PROM_NS" port-forward svc/"$PROM_SVC" 9090:9090 >/tmp/pf-prom2.log 2>&1 &
          echo $! > /tmp/pf_prom2.pid
          trap 'kill $(cat /tmp/pf_prom2.pid) 2>/dev/null || true' EXIT
          for i in {1..60}; do curl -sf http://127.0.0.1:9090/-/ready >/dev/null && break || sleep 1; done

          FEATURES=$(python3 - <<'PY'
          import json,requests,os
          ns=os.getenv("NS")
          label=os.getenv("SEL_LABEL","app")
          val=os.getenv("SEL_VALUE","backend")
          url="http://127.0.0.1:9090/api/v1/query"

          def q(e):
              return requests.get(url,params={'query':e},timeout=15).json()
          def val_of(js, default=0.0):
              try:
                  return float(js['data']['result'][0]['value'][1])
              except Exception:
                  return default

          lbl = "label_" + label.replace('.', '_').replace('/', '_')
          L = f'kube_pod_labels{{namespace="{ns}", {lbl}="{val}"}}'
          def sum_on_pod(expr):
              return f'sum(({expr}) * on (namespace,pod) group_left({lbl}) ({L}))'

          cpu = val_of(q(sum_on_pod(f'rate(container_cpu_usage_seconds_total{{namespace="{ns}",container!="POD",image!=""}}[2m])'))) * 100
          mem = val_of(q(sum_on_pod(f'container_memory_working_set_bytes{{namespace="{ns}",container!="POD",image!=""}}')))
          net = val_of(q(sum_on_pod(f'rate(container_network_receive_bytes_total{{namespace="{ns}",pod!=""}}[2m])')))
          restarts = val_of(q(sum_on_pod(f'increase(kube_pod_container_status_restarts_total{{namespace="{ns}"}}[5m])')))

          dep_lbl_sum = f'sum by (namespace, deployment, {lbl}) (kube_deployment_labels{{namespace="{ns}", {lbl}="{val}"}})'
          desired = val_of(q(f'sum(kube_deployment_spec_replicas{{namespace="{ns}"}} * on (namespace, deployment) group_left({lbl}) ({dep_lbl_sum}))'))
          ready   = val_of(q(f'sum(kube_deployment_status_ready_replicas{{namespace="{ns}"}} * on (namespace, deployment) group_left({lbl}) ({dep_lbl_sum}))'))
          unavail = val_of(q(f'sum(kube_deployment_status_replicas_unavailable{{namespace="{ns}"}} * on (namespace, deployment) group_left({lbl}) ({dep_lbl_sum}))'))
          ratio = (ready/desired) if desired>0 else 1.0

          http5xx = 0.0
          for num,den in [
            ('http_requests_total{code=~"5.."}','http_requests_total'),
            ('flask_http_request_total{status=~"5.."}','flask_http_request_total'),
          ]:
              try:
                  n = val_of(q(sum_on_pod(f'rate({num}[2m])')))
                  d = val_of(q('clamp_min('+sum_on_pod(f'rate({den}[2m])')+', 1)'))
                  if d>0 and n/d>0:
                      http5xx = n/d
                      break
              except Exception:
                  pass

          print(json.dumps({
            "restart_count_last_5m": restarts,
            "cpu_usage_pct": cpu,
            "memory_usage_bytes": mem,
            "ready_replica_ratio": round(ratio,4),
            "unavailable_replicas": unavail,
            "network_receive_bytes_per_s": net,
            "http_5xx_error_rate": http5xx
          }))
          PY
          )

          # call /predict again (service, not deployment)
          kubectl -n "${NS}" port-forward svc/${BACKEND_SERVICE} 5000:5000 >/tmp/pf-backend3.log 2>&1 &
          echo $! > /tmp/pf_backend3.pid
          for i in {1..60}; do curl -sf http://127.0.0.1:5000/healthz >/dev/null && break || sleep 1; done

          RESP=$(curl -s -X POST http://127.0.0.1:5000/predict \
                -H "Content-Type: application/json" -d "${FEATURES}")
          PROB=$(echo "$RESP" | sed -n 's/.*"probability":\s*\([0-9.]*\).*/\1/p'); : ${PROB:=0}
          echo "fail_prob2=$PROB" >> $GITHUB_OUTPUT

          kill $(cat /tmp/pf_backend3.pid) 2>/dev/null || true

          awk -v p="$PROB" -v thr="${THRESHOLD}" 'BEGIN{exit !(p>=thr)}' \
            && echo "still_high=true"  >> $GITHUB_OUTPUT \
            || echo "still_high=false" >> $GITHUB_OUTPUT

      - name: Rollback if still high
        if: ${{ steps.recheck.outputs.still_high == 'true' }}
        run: |
          kubectl -n "${EKS_NAMESPACE}" rollout undo deployment/${BACKEND_DEPLOY}

      # ---------- Slack notifications ----------
      - name: Slack alert (high → healed)
        if: ${{ steps.predict.outputs.highrisk == 'true' && steps.recheck.outputs.still_high != 'true' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -n "${SLACK_WEBHOOK_URL:-}" ]; then
            msg="⚠️ High risk (p=${{ steps.predict.outputs.fail_prob }}) on ${BACKEND_DEPLOY}. Auto-heal applied."
            printf '%s\n' "$msg" | jq -Rs '{text: .}' \
              | curl -sS -X POST -H 'Content-Type: application/json' --data @- "$SLACK_WEBHOOK_URL"
          fi

      - name: Slack alert (rolled back)
        if: ${{ steps.recheck.outputs.still_high == 'true' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -n "${SLACK_WEBHOOK_URL:-}" ]; then
            msg="⚠️ Risk persisted (p=${{ steps.recheck.outputs.fail_prob2 }}) → rolled back ${BACKEND_DEPLOY}."
            printf '%s\n' "$msg" | jq -Rs '{text: .}' \
              | curl -sS -X POST -H 'Content-Type: application/json' --data @- "$SLACK_WEBHOOK_URL"
          fi

      - name: Slack success (kept)
        if: ${{ steps.predict.outputs.highrisk != 'true' }}
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          if [ -n "${SLACK_WEBHOOK_URL:-}" ]; then
            msg="✅ Rollout kept. Risk p=${{ steps.predict.outputs.fail_prob }}."
            printf '%s\n' "$msg" | jq -Rs '{text: .}' \
              | curl -sS -X POST -H 'Content-Type: application/json' --data @- "$SLACK_WEBHOOK_URL"
          fi
